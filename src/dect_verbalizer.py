from inspect import Parameter
import json
import time
from os import stat
import os
from transformers.file_utils import ModelOutput
from transformers.tokenization_utils import PreTrainedTokenizer
from transformers.utils.dummy_pt_objects import PreTrainedModel
from openprompt.data_utils import InputExample, InputFeatures
import re
from openprompt import Verbalizer
from typing import *
import torch
import torch.nn as nn
import torch.nn.functional as F
import copy
from transformers.modeling_outputs import CausalLMOutputWithCrossAttentions, Seq2SeqLMOutput, MaskedLMOutput

class DecTVerbalizer(Verbalizer):
    r"""
    L·ªõp th·ª±c hi·ªán Verbalizer trong `Prototypical Verbalizer for Prompt-based Few-shot Tuning`

    Args:   
        tokenizer (:obj:`PreTrainedTokenizer`): B·ªô m√£ h√≥a c·ªßa m√¥ h√¨nh pre-trained hi·ªán t·∫°i ƒë·ªÉ ch·ªâ ƒë·ªãnh t·ª´ v·ª±ng.
        classes (:obj:`List[Any]`): C√°c l·ªõp (ho·∫∑c nh√£n) c·ªßa nhi·ªám v·ª• hi·ªán t·∫°i.
        label_words (:obj:`Union[List[str], List[List[str]], Dict[List[str]]]`, optional): C√°c t·ª´ nh√£n ƒë∆∞·ª£c √°nh x·∫° b·ªüi c√°c nh√£n.
        prefix (:obj:`str`, optional): Chu·ªói ti·ªÅn t·ªë c·ªßa verbalizer (d√πng trong PLM nh∆∞ RoBERTa, nh·∫°y v·ªõi kho·∫£ng tr·∫Øng ti·ªÅn t·ªë).
        multi_token_handler (:obj:`str`, optional): Chi·∫øn l∆∞·ª£c x·ª≠ l√Ω cho nhi·ªÅu token ƒë∆∞·ª£c t·∫°o b·ªüi tokenizer.
        post_log_softmax (:obj:`bool`, optional): C√≥ √°p d·ª•ng log softmax sau x·ª≠ l√Ω tr√™n label_logits kh√¥ng. M·∫∑c ƒë·ªãnh l√† True.
        lr: (:obj:`float`, optional): T·ªëc ƒë·ªô h·ªçc cho prototype.
        hidden_size: (:obj:`int`, optional): K√≠ch th∆∞·ªõc c·ªßa tr·∫°ng th√°i ·∫©n c·ªßa m√¥ h√¨nh.
        mid_dim: (:obj:`int`, optional): K√≠ch th∆∞·ªõc c·ªßa embedding prototype.
        epochs: (:obj:`int`, optional): S·ªë epoch hu·∫•n luy·ªán prototype.
        model_logits_weight: (:obj:`float`, optional): H·ªá s·ªë tr·ªçng s·ªë (\lambda) cho model logits.
    """
    def __init__(self, 
                tokenizer: Optional[PreTrainedTokenizer],
                classes: Optional[List] = None,
                num_classes: Optional[int] = None,
                label_words: Optional[Union[Sequence[str], Mapping[str, str]]] = None,
                prefix: Optional[str] = "",
                multi_token_handler: Optional[str] = "first",
                post_log_softmax: Optional[bool] = True,
                lr: Optional[float] = 1e-3,
                hidden_size: Optional[int] = 1024,
                mid_dim: Optional[int] = 64,
                epochs: Optional[int] = 5,
                model_logits_weight: Optional[float] = 1,
                save_dir: Optional[str] = None,
                model: Optional[nn.Module] = None
                ):
        
        super().__init__(tokenizer=tokenizer, num_classes=num_classes, classes=classes)
        
        self.prefix = prefix
        self.multi_token_handler = multi_token_handler
        self.post_log_softmax = post_log_softmax
        self.lr = lr
        self.mid_dim = mid_dim
        self.epochs = epochs
        self.model_logits_weight = model_logits_weight
        self.save_dir = save_dir
        self.hidden_dims = hidden_size
        self.num_label_words_per_class = None

        # Kh·ªüi t·∫°o projection head
        self.head = nn.Linear(self.hidden_dims, self.mid_dim, bias=False)

        # N·∫øu m√¥ h√¨nh d√πng float16 ‚Üí √©p dtype cho self.head
        if model is not None and next(model.parameters()).dtype == torch.float16:
            self.head = self.head.half()
            self.proto = self.proto.half()
            self.proto_r = self.proto_r.half()

        # N·∫øu c√≥ label words ƒë·ªÉ kh·ªüi t·∫°o prototype
        if label_words is not None:
            self.label_words = label_words

        # Proto vector
        w = torch.empty((self.num_classes, self.mid_dim))
        nn.init.xavier_uniform_(w)
        self.proto = nn.Parameter(w, requires_grad=False)

        # B√°n k√≠nh c·ªßa prototype
        r = torch.ones(self.num_classes)
        self.proto_r = nn.Parameter(r, requires_grad=True)

        # Optimizer
        self.optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)

    @property
    def group_parameters_proto(self,):
        r"""Bao g·ªìm c√°c tham s·ªë c·ªßa l·ªõp cu·ªëi c√πng
        """
        return [p for n, p in self.head.named_parameters()] + [self.proto_r]

    def on_label_words_set(self):
        self.label_words = self.add_prefix(self.label_words, self.prefix)
        self.generate_parameters()
        
    @staticmethod
    def add_prefix(label_words, prefix):
        r"""Th√™m ti·ªÅn t·ªë v√†o c√°c t·ª´ nh√£n. V√≠ d·ª•, n·∫øu m·ªôt t·ª´ nh√£n n·∫±m gi·ªØa template,
        ti·ªÅn t·ªë n√™n l√† ``' '``.

        Args:
            label_words (:obj:`Union[Sequence[str], Mapping[str, str]]`, optional): C√°c t·ª´ nh√£n ƒë∆∞·ª£c √°nh x·∫° b·ªüi c√°c nh√£n.
            prefix (:obj:`str`, optional): Chu·ªói ti·ªÅn t·ªë c·ªßa verbalizer.
        
        Returns:
            :obj:`Sequence[str]`: C√°c t·ª´ nh√£n m·ªõi k√®m ti·ªÅn t·ªë.
        """
        new_label_words = []
        if isinstance(label_words[0], str):
            label_words = [[w] for w in label_words]  # Bao b·ªçc th√†nh danh s√°ch c√°c danh s√°ch t·ª´ nh√£n.

        for label_words_per_label in label_words:
            new_label_words_per_label = []
            for word in label_words_per_label:
                if word.startswith("<!>"):
                    new_label_words_per_label.append(word.split("<!>")[1])
                else:
                    new_label_words_per_label.append(prefix + word)
            new_label_words.append(new_label_words_per_label)
        return new_label_words

    def generate_parameters(self) -> List:
        r"""Trong template th·ªß c√¥ng c∆° b·∫£n, c√°c tham s·ªë ƒë∆∞·ª£c t·∫°o tr·ª±c ti·∫øp t·ª´ c√°c t·ª´ nh√£n.
        Trong tri·ªÉn khai n√†y, c√°c t·ª´ nh√£n kh√¥ng n√™n ƒë∆∞·ª£c m√£ h√≥a th√†nh nhi·ªÅu h∆°n m·ªôt token.
        """
        all_ids = []
        for words_per_label in self.label_words:
            ids_per_label = []
            for word in words_per_label:
                ids = self.tokenizer.encode(word, add_special_tokens=False)
                ids_per_label.append(ids)
            all_ids.append(ids_per_label)

        max_len = max([max([len(ids) for ids in ids_per_label]) for ids_per_label in all_ids])
        max_num_label_words = max([len(ids_per_label) for ids_per_label in all_ids])
        self.num_label_words_per_class = max_num_label_words
        words_ids_mask = torch.zeros(max_num_label_words, max_len)
        words_ids_mask = [[[1]*len(ids) + [0]*(max_len-len(ids)) for ids in ids_per_label]
                             + [[0]*max_len]*(max_num_label_words-len(ids_per_label)) 
                             for ids_per_label in all_ids]
        words_ids = [[ids + [0]*(max_len-len(ids)) for ids in ids_per_label]
                             + [[0]*max_len]*(max_num_label_words-len(ids_per_label)) 
                             for ids_per_label in all_ids]
        
        words_ids_tensor = torch.tensor(words_ids)
        words_ids_mask = torch.tensor(words_ids_mask)
        self.label_words_ids = nn.Parameter(words_ids_tensor, requires_grad=False)
        self.words_ids_mask = nn.Parameter(words_ids_mask, requires_grad=False) # M·∫∑t n·∫° 3 chi·ªÅu
        self.label_words_mask = nn.Parameter(torch.clamp(words_ids_mask.sum(dim=-1), max=1), requires_grad=False)

    def process_hiddens(self, hiddens: torch.Tensor, model_logits, **kwargs):
        r"""Khung x·ª≠ l√Ω to√†n b·ªô logits g·ªëc tr√™n t·ª´ v·ª±ng, g·ªìm b·ªën b∆∞·ªõc:
        """
        proto_logits = self.sim(self.head(hiddens), self.proto, self.proto_r, model_logits, self.model_logits_weight)
        return proto_logits

    def project(self, logits: torch.Tensor, **kwargs) -> torch.Tensor:
        print("üîç H√¨nh d·∫°ng logits:", logits.shape)

        # L·ªçc b·ªè c√°c token ƒë·ªám (s·ªë 0) t·ª´ label_words_ids
        label_ids = []
        for ids_per_label in self.label_words_ids:
            for ids in ids_per_label:
                # Ch·ªâ bao g·ªìm c√°c ID token kh√°c 0
                valid_ids = [id for id in ids if id != 0]
                if valid_ids:
                    label_ids.extend(valid_ids)

        print("‚úÖ S·ª≠ d·ª•ng c√°c ID token nh√£n:", label_ids)

        if logits.ndim < 2 or logits.shape[1] == 0:
            print("‚ö†Ô∏è Logits kh√¥ng h·ª£p l·ªá: shape nh·ªè h∆°n 2 chi·ªÅu ho·∫∑c chi·ªÅu th·ª© 2 b·∫±ng 0.")
            return torch.empty(logits.shape[0], self.num_classes, device=logits.device, dtype=logits.dtype)

        # Ch·ªçn logits cho c√°c ID token nh√£n h·ª£p l·ªá
        label_words_logits = logits[:, label_ids]
        return label_words_logits

    def process_logits(self, logits: torch.Tensor, **kwargs):
        r"""Khung x·ª≠ l√Ω to√†n b·ªô logits g·ªëc tr√™n t·ª´ v·ª±ng, g·ªìm b·ªën b∆∞·ªõc:

        (1) √Ånh x·∫° logits th√†nh logits c·ªßa c√°c t·ª´ nh√£n

        n·∫øu self.post_log_softmax l√† True:

            (2) Chu·∫©n h√≥a tr√™n t·∫•t c·∫£ c√°c t·ª´ nh√£n

            (3) Hi·ªáu ch·ªânh (t√πy ch·ªçn)

        (4) T·ªïng h·ª£p (cho nhi·ªÅu t·ª´ nh√£n)

        Args:
            logits (:obj:`torch.Tensor`): Logits g·ªëc.
        
        Returns:
            (:obj:`torch.Tensor`): Logits cu·ªëi c√πng ƒë√£ x·ª≠ l√Ω tr√™n c√°c nh√£n (l·ªõp).
        """
        # √Ånh x·∫°
        if logits.ndim < 2 or logits.shape[1] == 0:
            print("‚ö†Ô∏è Logits kh√¥ng h·ª£p l·ªá: shape nh·ªè h∆°n 2 chi·ªÅu ho·∫∑c chi·ªÅu th·ª© 2 b·∫±ng 0.")
            return torch.empty(logits.shape[0], self.num_classes, device=logits.device, dtype=logits.dtype)

        label_words_logits = self.project(logits, **kwargs)

        if self.post_log_softmax:
            if hasattr(self, "_calibrate_logits") and self._calibrate_logits is not None:
                # ƒê·∫£m b·∫£o shape kh·ªõp
                if self._calibrate_logits.shape[1:] == label_words_logits.shape[1:]:
                    label_words_logits = self.calibrate(label_words_probs=label_words_logits)
                else:
                    print("‚ö†Ô∏è B·ªè qua hi·ªáu ch·ªânh v√¨ shape kh√¥ng kh·ªõp:",
                        f"{self._calibrate_logits.shape} vs {label_words_logits.shape}")

        label_logits = self.aggregate(label_words_logits)
        if label_logits.dim() == 1:
            label_logits = label_logits.unsqueeze(0)  # Chuy·ªÉn [K] -> [1, K]
        return label_logits
    
    def normalize(self, logits: torch.Tensor) -> torch.Tensor:
        """
        Cho logits tr√™n to√†n b·ªô t·ª´ v·ª±ng, tr·∫£ v·ªÅ x√°c su·∫•t tr√™n t·∫≠p c√°c t·ª´ nh√£n.
        
        Args:
            logits (:obj:`Tensor`): Logits tr√™n to√†n b·ªô t·ª´ v·ª±ng.

        Returns:
            :obj:`Tensor`: Logits tr√™n t·∫≠p c√°c t·ª´ nh√£n.
        """
        batch_size = logits.shape[0]
        return F.softmax(logits.reshape(batch_size, -1), dim=-1).reshape(*logits.shape)

    def aggregate(self, label_words_logits: torch.Tensor) -> torch.Tensor:
        # T√≠nh s·ªë l∆∞·ª£ng ID token h·ª£p l·ªá cho m·ªói l·ªõp
        valid_ids_per_class = [sum(1 for id in ids if id != 0) for ids in self.label_words_ids.view(self.num_classes, -1)]
        max_valid_ids = max(valid_ids_per_class)
        
        # Kh·ªüi t·∫°o tensor ƒë·∫ßu ra
        batch_size = label_words_logits.shape[0]
        aggregated = torch.zeros(batch_size, self.num_classes, device=label_words_logits.device, dtype=label_words_logits.dtype)
        
        # Ch·ªâ s·ªë ƒë·ªÉ theo d√µi v·ªã tr√≠ trong label_words_logits
        idx = 0
        for i in range(self.num_classes):
            num_valid_ids = valid_ids_per_class[i]
            if num_valid_ids > 0:
                # Tr√≠ch xu·∫•t logits cho l·ªõp n√†y
                class_logits = label_words_logits[:, idx:idx + num_valid_ids]
                # T·ªïng h·ª£p b·∫±ng c√°ch l·∫•y trung b√¨nh c√°c logits h·ª£p l·ªá
                aggregated[:, i] = class_logits.mean(dim=-1)
                idx += num_valid_ids

        return aggregated

    def calibrate(self, label_words_probs: torch.Tensor, **kwargs) -> torch.Tensor:
        r"""
        
        Args:
            label_words_probs (:obj:`torch.Tensor`): Ph√¢n ph·ªëi x√°c su·∫•t c·ªßa c√°c t·ª´ nh√£n v·ªõi h√¨nh d·∫°ng [``batch_size``, ``num_classes``, ``num_label_words_per_class``]
        
        Returns:
            :obj:`torch.Tensor`: X√°c su·∫•t ƒë√£ hi·ªáu ch·ªânh c·ªßa c√°c t·ª´ nh√£n.
        """
        shape = label_words_probs.shape
        calibrate_label_words_probs = self._calibrate_logits
        if calibrate_label_words_probs.dim() == 2 and label_words_probs.dim() == 3:
            calibrate_label_words_probs = calibrate_label_words_probs.unsqueeze(-1)

        assert calibrate_label_words_probs.shape[1:] == label_words_probs.shape[1:] \
             and calibrate_label_words_probs.shape[0]==1, "shape kh√¥ng kh·ªõp"
        label_words_probs /= (calibrate_label_words_probs+1e-15)
        
        return label_words_probs

    def process_outputs(self, outputs: Union[torch.Tensor, torch.Tensor], batch: Union[Dict, InputFeatures], **kwargs):
        model_logits = self.process_logits(outputs[1])
        proto_logits = self.process_hiddens(outputs[0], model_logits)
        return proto_logits

    def gather_outputs(self, outputs: ModelOutput):
        logits = outputs.logits
        if isinstance(outputs, Seq2SeqLMOutput):
            ret = outputs.decoder_hidden_states[-1]
        elif isinstance(outputs, MaskedLMOutput) or isinstance(outputs, CausalLMOutputWithCrossAttentions):
            ret = outputs.hidden_states[-1]
        else:
            try:
                ret = outputs.hidden_states[-1]
            except AttributeError:
                raise NotImplementedError(f"Ph∆∞∆°ng th·ª©c gather outputs cho lo·∫°i outputs {type(outputs)} ch∆∞a ƒë∆∞·ª£c tri·ªÉn khai")

        return ret, logits

    @staticmethod
    def sim(x, y, r=0, model_logits=0, model_logits_weight=1):
        if x.dim() == 3:
            x = x.mean(dim=1)
        
        x = F.normalize(x, p=2, dim=-1)
        y = F.normalize(y, p=2, dim=-1)
        
        sim_matrix = torch.mm(x, y.t())
        
        # ƒê·∫£m b·∫£o model_logits kh·ªõp v·ªõi k√≠ch th∆∞·ªõc sim_matrix
        if isinstance(model_logits, torch.Tensor):
            if model_logits.dim() == 1:
                model_logits = model_logits.unsqueeze(-1)
            if model_logits.shape[0] != sim_matrix.shape[0]:
                model_logits = model_logits[:sim_matrix.shape[0]]
        
        # X·ª≠ l√Ω chi·ªÅu c·ªßa r
        if isinstance(r, torch.Tensor) and r.dim() == 0:
            r = r.unsqueeze(0)
        
        # T√≠nh k·∫øt qu·∫£
        result = sim_matrix - model_logits * model_logits_weight - r
        return -result

    def loss_func(self, x, model_logits, labels):
        assert model_logits.shape[1] == self.num_classes, \
            f"‚ùå H√¨nh d·∫°ng model_logits {model_logits.shape} kh√¥ng kh·ªõp num_classes = {self.num_classes}"
        sim_mat = torch.exp(self.sim(x, self.proto, self.proto_r, model_logits, self.model_logits_weight))
        pos_score = torch.sum(sim_mat * F.one_hot(labels), -1)
        loss = -torch.mean(torch.log(pos_score / sim_mat.sum(-1)))
        
        return loss

    def test(self, model, dataloader):
        batch_size = dataloader.batch_size
        model.eval()
        
        if dataloader is None:
            print("‚ùå L·ªói: Test dataloader kh√¥ng t·ªìn t·∫°i (None)")
            return [], [], []
        
        if len(dataloader) == 0:
            print("‚ùå L·ªói: Test dataloader r·ªóng (0 samples)")
            return [], [], []
        
        print(f"üîç B·∫Øt ƒë·∫ßu inference v·ªõi {len(dataloader)} batch...")
        model_preds, preds, labels = [], [], []
        
        if os.path.isfile(f"{self.save_dir}/logits.pt"):
            print("üìÇ ƒêang t·∫£i logits v√† hiddens t·ª´ cache...")
            logits = torch.load(f"{self.save_dir}/logits.pt")
            hiddens = torch.load(f"{self.save_dir}/hiddens.pt")
            
            for i, batch in enumerate(dataloader):
                print(f"  ‚Üí ƒêang x·ª≠ l√Ω batch {i+1}/{len(dataloader)}")
                try:
                    batch = batch.cuda().to_dict()
                    length = len(batch['label'])
                    labels.extend(batch.pop('label').cpu().tolist())
                    batch_hidden = hiddens[i*batch_size: i*batch_size+length].to(self.head.weight.dtype)
                    batch_logits = logits[i*batch_size: i*batch_size+length].to(self.head.weight.dtype)
                    proto = self.process_hiddens(batch_hidden, batch_logits)
                    model_pred = torch.argmax(batch_logits, dim=-1)
                    pred = torch.argmax(proto, dim=-1)
                    preds.extend(pred.cpu().tolist())
                    model_preds.extend(model_pred.cpu().tolist())
                except Exception as e:
                    print(f"‚ùå L·ªói khi x·ª≠ l√Ω batch {i+1}: {str(e)}")
                    import traceback
                    traceback.print_exc()
        else:
            logits, hiddens = [], []
            print("üß† ƒêang t√≠nh to√°n logits v√† hiddens m·ªõi...")
            with torch.no_grad(), torch.cuda.amp.autocast():
                total_batches = len(dataloader)
                for i, batch in enumerate(dataloader):
                    print(f"  ‚Üí ƒêang x·ª≠ l√Ω batch {i+1}/{total_batches}")
                    try:
                        batch = batch.cuda().to_dict()
                        labels.extend(batch.pop('label').cpu().tolist())
                        outputs = model.prompt_model(batch)
                        outputs = self.gather_outputs(outputs)
                        batch_hidden = outputs[0][:, -1, :].to(self.head.weight.dtype)
                        batch_logits = outputs[1][:, -1, :].to(self.head.weight.dtype)
                        if batch_logits.shape[1] == 0:
                            print(f"‚ö†Ô∏è B·ªè qua batch {i+1} v√¨ kh√¥ng c√≥ token ƒë·ªÉ tr√≠ch xu·∫•t.")
                            continue
                        model_logits = self.process_logits(batch_logits)
                        logits.append(model_logits)
                        hiddens.append(batch_hidden)
                        proto = self.process_hiddens(batch_hidden, model_logits)
                        model_pred = torch.argmax(model_logits, dim=-1)
                        pred = torch.argmax(proto, dim=-1)
                        preds.extend(pred.cpu().tolist())
                        model_preds.extend(model_pred.cpu().tolist())
                    except Exception as e:
                        print(f"‚ùå L·ªói khi x·ª≠ l√Ω batch {i+1}: {str(e)}")
                        import traceback
                        traceback.print_exc()
                        continue
                
                if logits:
                    logits = torch.cat(logits, dim=0)
                    hiddens = torch.cat(hiddens, dim=0)
                    
                    os.makedirs(self.save_dir, exist_ok=True)
                    
                    print(f"üíæ ƒêang l∆∞u logits v√† hiddens v√†o {self.save_dir}")
                    torch.save(logits, f"{self.save_dir}/logits.pt")
                    torch.save(hiddens, f"{self.save_dir}/hiddens.pt")
                else:
                    print("‚ö†Ô∏è Kh√¥ng c√≥ logits n√†o ƒë∆∞·ª£c t·∫°o do l·ªói")
        
        print(f"‚úÖ ƒê√£ ho√†n th√†nh inference: {len(labels)} m·∫´u")
        return model_preds, preds, labels

    def train_proto(self, model, dataloader, calibrate_dataloader):
        print("üß† Verbalizer.label_words_ids:", self.label_words_ids)
        
        if hasattr(model, "tokenizer"):
            print("üìö K√≠ch th∆∞·ªõc t·ª´ v·ª±ng t·ª´ tokenizer:", model.tokenizer.vocab_size)
        else:
            print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y tokenizer trong m√¥ h√¨nh")

        model.eval()

        embeds = [[] for _ in range(self.num_classes)]
        labels = [[] for _ in range(self.num_classes)]
        model_logits_all = []

        total_num = 0
        start_time = time.time()

        all_embeds = []
        all_labels = []
        all_model_logits = []

        with torch.no_grad():
            # Hi·ªáu ch·ªânh logits
            if calibrate_dataloader is not None:
                for i, batch in enumerate(calibrate_dataloader):
                    batch = batch.cuda().to_dict()
                    outputs = model.prompt_model(batch)
                    outputs = self.gather_outputs(outputs)
                    logits = self.project(outputs[1][:, -1, :].to(self.head.weight.dtype))
                    self._calibrate_logits = logits / torch.mean(logits)

            # Thu th·∫≠p d·ªØ li·ªáu prototype
            for i, batch in enumerate(dataloader):
                batch = batch.cuda().to_dict()
                outputs = self.gather_outputs(model.prompt_model(batch))

                label_batch = batch['label']
                batch_size = len(label_batch)

                hidden = outputs[0][:batch_size, -1, :].to(self.head.weight.dtype)
                logits = outputs[1][:batch_size, -1, :].to(self.head.weight.dtype)

                processed_logits = self.process_logits(logits)
                total_num += batch_size
                all_embeds.append(hidden)
                all_labels.append(label_batch)
                all_model_logits.append(processed_logits)
                
        if all_embeds:
            all_embeds_tensor = torch.cat(all_embeds, dim=0)
            all_labels_tensor = torch.cat(all_labels, dim=0)
            all_model_logits_tensor = torch.cat(all_model_logits, dim=0)
        else:
            print("‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ hu·∫•n luy·ªán prototype")
            return

        # T√≠nh b√°n k√≠nh prototype
        dist = []
        for x in embeds:
            if len(x) > 0:  # B·ªè qua c√°c l·ªõp r·ªóng
                x = torch.stack(x).to(self.head.weight.dtype)
                center = x.mean(0, keepdim=True)
                projected = self.head(x)
                center_proj = self.head(center)
                d = torch.norm(projected - center_proj, dim=-1).mean()
                dist.append(d)
        if dist:
            self.proto_r.data = torch.stack(dist)

        # Hu·∫•n luy·ªán prototype
        print(f"üî• B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán prototype trong {self.epochs} epoch...")
        for epoch in range(self.epochs):
            # √Ånh x·∫° embedding
            x = self.head(all_embeds_tensor.to(self.head.weight.dtype))
        
            self.optimizer.zero_grad()
            loss = self.loss_func(x, all_model_logits_tensor, all_labels_tensor)
            loss.backward()
            self.optimizer.step()
            print(f"Epoch {epoch+1}/{self.epochs}, Loss: {loss.item():.4f}")
            torch.cuda.empty_cache()

        print("T·ªïng s·ªë epoch: {}. M·∫•t m√°t DecT: {}".format(self.epochs, loss.item()))
        print("Th·ªùi gian hu·∫•n luy·ªán: {}".format(time.time() - start_time))